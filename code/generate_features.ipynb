{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation\n",
    "\n",
    "This is the core of COLOGNE where we generate discrete embeddings per node. Below we have implemented the three methods for feature generation: \n",
    "- Random walks\n",
    "- NodeSketch\n",
    "- Minwise independent sampling (L0)\n",
    "- L1 sampling\n",
    "- L2 sampling\n",
    "\n",
    "In order to run the code the graphs must be preprocessed, please run first the respective notebooks.\n",
    "More details are provided as comments in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphnames = ['Cora', 'Citeseer', 'Pubmed', 'HomoSapiens', 'Wikipedia', 'BlogCatalog']\n",
    "idx = 0\n",
    "graphname = graphnames[idx]\n",
    "emb_size = 50 # how many samples per node to generate \n",
    "depth = 4 # the depth of the local neighborhood\n",
    "data_dir = os.path.expanduser(\"../Graphs/\"+graphname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodedata_path = data_dir + \"/data/nodedata.json\" \n",
    "with open(nodedata_path, \"r\") as read_file:\n",
    "    nodedata = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label=Genetic_Algorithms',\n",
       " {'w-19': 1, 'w-299': 1, 'w-393': 1, 'w-495': 1, 'w-507': 1, 'w-1263': 1}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example of node information. The node label and a dictionary of words (with weights) describing the node \n",
    "nodedata[list(nodedata.keys())[129]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph_from_edge_list(filename, nodedata):\n",
    "    G = nx.Graph()\n",
    "    path = data_dir + \"/data/\" + filename\n",
    "    cnt = 0\n",
    "    with open(path, 'r') as edgefile: # os.path.join(data_dir, filename),\n",
    "        for line in edgefile:\n",
    "            cnt += 1\n",
    "            line_split = line.split(':')\n",
    "            if len(line_split) > 1:\n",
    "                l0 = line_split[0]\n",
    "                l1 = line_split[1]\n",
    "                u = l0.strip()\n",
    "                v = l1.strip()\n",
    "                if u in nodedata and v in nodedata:\n",
    "                    G.add_edge(u, v)\n",
    "        \n",
    "    print(cnt)\n",
    "    return G\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5278\n"
     ]
    }
   ],
   "source": [
    "G = read_graph_from_edge_list(\"all_graph_edges.txt\", nodedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 5278)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222\n"
     ]
    }
   ],
   "source": [
    "# the graph used for link prediction\n",
    "H = read_graph_from_edge_list(\"graph_edges_reduced.txt\", nodedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 4222)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.number_of_nodes(), H.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056\n"
     ]
    }
   ],
   "source": [
    "R = read_graph_from_edge_list(\"removed_edges.txt\", nodedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1281, 1056)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.number_of_nodes(), R.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(label):\n",
    "    if label[:4] == 'None':\n",
    "        return []\n",
    "    else:\n",
    "        labels = label.split('=')\n",
    "        return labels[1:len(labels)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random walk sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random int in [start, end]\n",
    "def get_rnd_int_in_range(start, end):\n",
    "    r = random.randint(0, 1e10)\n",
    "    diff = end - start + 1\n",
    "    rval = r%diff\n",
    "    return rval+start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{8: 10015, 3: 9967, 0: 9930, 7: 10046, 5: 9980, 9: 10043, 2: 9980, 4: 9869, 1: 10168, 6: 10002}\n"
     ]
    }
   ],
   "source": [
    "test_cnts = {}\n",
    "for i in range(100000):\n",
    "    val = get_rnd_int_in_range(0, 9)\n",
    "    test_cnts.setdefault(val, 0)\n",
    "    test_cnts[val] += 1\n",
    "print(test_cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random value in (min_val, 1]\n",
    "def get_rnd_value(min_val):\n",
    "    if min_val >=1:\n",
    "        raise Exception(\"Minimum must be less than 1\")\n",
    "    rval = 0\n",
    "    while rval < min_val:\n",
    "        rval = random.random()\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6062148519886823"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rnd_value(1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a standard random walk starting from a node for 'depth' hops \n",
    "def random_walk(G, node, depth, features):\n",
    "    node = str(node)\n",
    "    cnt = 0\n",
    "    curr_node = node\n",
    "    while cnt < depth and G.degree[curr_node] > 0:\n",
    "        nbrs = [curr_node] + [nbr for nbr in G.neighbors(curr_node)]\n",
    "        curr_node = nbrs[get_rnd_int_in_range(0, len(nbrs)-1)]\n",
    "        cnt += 1\n",
    "    subject, features_node = features[curr_node]\n",
    "    # return a random feature describing the node\n",
    "    if len(features_node.values())==0:\n",
    "        print(features[curr_node])\n",
    "    random.seed(get_rnd_int_in_range(0, len(G.nodes())))    \n",
    "    w = random.choices(population=list(features_node.keys()), weights=list(features_node.values()), k=1)[0]\n",
    "    return curr_node, subject, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('35', 'label=Genetic_Algorithms', 'w-1249')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = list(H.nodes())[40]\n",
    "random_walk(H, node, 2, features=nodedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each node generate a number of samples, i.e. the embedding size, by random walks\n",
    "def all_nodes_random_walk(G, depth, nr_walks, features):\n",
    "    vectors = {}\n",
    "    for node in G.nodes():\n",
    "        vectors[node] = [None for _ in range(nr_walks)]\n",
    "        for walk in range(nr_walks):\n",
    "            sample, subject, feature = random_walk(G, node, depth, features)\n",
    "            vectors[node][walk] = (sample, subject, feature)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time RW 2.0955722332000732\n",
      "Elapsed time RW 2.443809747695923\n",
      "Elapsed time RW 3.187406063079834\n",
      "Elapsed time RW 4.012030124664307\n",
      "Elapsed time RW 4.627681016921997\n"
     ]
    }
   ],
   "source": [
    "# random walks on the full graph\n",
    "vectors_rw_all = []\n",
    "for d in range(depth+1):\n",
    "    start = time.time()\n",
    "    vectors_rw = all_nodes_random_walk(G, d, emb_size, features=nodedata)\n",
    "    vectors_rw_all.append(vectors_rw)\n",
    "    end = time.time()\n",
    "    print('Elapsed time RW', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random walks on the reduced graph used for link prediction\n",
    "vectors_rw_reduced = [] \n",
    "for d in range(depth+1):\n",
    "    vectors_rw = all_nodes_random_walk(H, d, emb_size, features=nodedata)\n",
    "    vectors_rw_reduced.append(vectors_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('248425', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-1332'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-507'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-464'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-191'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-1305'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-1247'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-748'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-507'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-326'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-1353'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-25'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-1235'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-478'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-580'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-1227'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-1353'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-507'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-1247'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-140'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-911'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-1215'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-429'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-748'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-1235'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-442'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-660'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-442'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-1353'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-1215'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-1332'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-495'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-507'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-464'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-191'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-1305'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-1247'),\n",
       " ('35', 'label=Genetic_Algorithms', 'w-748'),\n",
       " ('248431', 'label=Genetic_Algorithms', 'w-507'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-326'),\n",
       " ('248425', 'label=Genetic_Algorithms', 'w-1353'),\n",
       " ('1113831', 'label=Genetic_Algorithms', 'w-25')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_rw_all[1][list(vectors_rw_all[0].keys())[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_rwalk_all_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_rw_all[d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_rwalk_reduced_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_rw_reduced[d], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodesketch sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(d, k, min_val, seed):\n",
    "    random.seed(seed)\n",
    "    if k not in d:\n",
    "        d[k] = random.random() #get_rnd_value(rand_gen, min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioffe_sampling(arr, weights):\n",
    "    min_val = 1e6\n",
    "    node_sample = None\n",
    "    feature_sample = None\n",
    "    weight_sample = 0\n",
    "    label_sample = None\n",
    "    #for node, feature, weight, label in arr:\n",
    "    for node, vals in arr.items():\n",
    "        feature, weight, label = vals[0], vals[1], vals[2]\n",
    "        rnd_val = -math.log(weights[node])/weight\n",
    "        if rnd_val < min_val:\n",
    "            min_val = rnd_val\n",
    "            node_sample = node\n",
    "            feature_sample = feature\n",
    "            weight_sample = weight\n",
    "            label_sample = label\n",
    "    return node_sample, feature_sample, weight_sample, label_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_arr(arr, new_node):\n",
    "    if new_node[0] in arr:\n",
    "        arr[new_node[0]] = (new_node[1], arr[new_node[0]][1] + new_node[2], new_node[3])# [1] += new_node[2]\n",
    "    else:\n",
    "        arr[new_node[0]] = (new_node[1], new_node[2], new_node[3])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodesketch_iter(G, nodedata, depth, emb_size):\n",
    "    \n",
    "    min_val = 1e-6\n",
    "    feats_rnd = [{} for _ in range(emb_size)]\n",
    "    cnt = 0\n",
    "    for i in range(emb_size):\n",
    "        feats_rnd_i = feats_rnd[i]\n",
    "        for node, feats in nodedata.items():\n",
    "            for f, weight_f in feats[1].items():\n",
    "                cnt += 1\n",
    "                update_dict(feats_rnd_i, f, min_val, seed=13*cnt)\n",
    "                \n",
    "    node_labels = [{} for _ in range(emb_size)]\n",
    "    for i in range(emb_size):\n",
    "        node_labels_i = node_labels[i]\n",
    "        feats_rnd_i = feats_rnd[i]\n",
    "        for node, feats in nodedata.items():\n",
    "            arr = {}\n",
    "            for f, weight_f in feats[1].items():\n",
    "                arr[f] = (f, weight_f, feats[0])\n",
    "                #arr.append((f, f, weight_f, feats[0]))\n",
    "            _, feature_sample, weight_sample, label_sample = ioffe_sampling(arr, feats_rnd_i)\n",
    "            node_labels_i[node] = (node, feature_sample, weight_sample, label_sample)\n",
    "            \n",
    "    print('Sampled features')\n",
    "    \n",
    "    node_rnd_vals_all = [{} for _ in range(emb_size)]\n",
    "    for t in range(emb_size):\n",
    "        random.seed(1223*t)\n",
    "        for u in G.nodes():\n",
    "            node_rnd_vals_all[t][u] = random.random()\n",
    "            \n",
    "    node_labels_all = [[{} for _ in range(emb_size)] for _ in range(depth+1)]\n",
    "    node_labels_all[0] = node_labels\n",
    "    for d in range(depth):\n",
    "        node_labels_iter = node_labels_all[d]\n",
    "        print('Iteration', d)\n",
    "        random.seed(31*d)\n",
    "        # node_rnd_vals = {}\n",
    "        \n",
    "        for t in range(emb_size):\n",
    "            node_labels_iter_t = node_labels_iter[t]\n",
    "            #node_rnd_vals = {}\n",
    "            #node_rnd_vals_t = node_rnd_vals_all[t]\n",
    "#             random.seed(31*d + 1223*t)\n",
    "#             for u in G.nodes():\n",
    "#                 node_rnd_vals[u] = random.random()# get_rnd_value(rand_gen, min_val)\n",
    "            # print(node_rnd_vals['35'])\n",
    "            for u in G.nodes():\n",
    "                node_sample_u, feature_sample_u, weight_sample_u, label_u = node_labels_iter_t[u]\n",
    "                arr_u = {node_sample_u: (feature_sample_u, weight_sample_u, label_u)} \n",
    "                #[(node_sample_u, feature_sample_u, weight_sample_u, label_u)]\n",
    "                for v in G.neighbors(u):\n",
    "                    node_sample_v, feature_sample_v, weight_sample_v, label_v = node_labels_iter_t[v]\n",
    "                    update_arr(arr_u, (node_sample_v, feature_sample_v, weight_sample_v, label_v))\n",
    "                    #arr_u.append((node_sample_v, feature_sample_v, weight_sample_v, label_v))\n",
    "                # sample_u, weight_sample_u = ioffe_sampling(arr_u, node_rnd_vals)\n",
    "                node_labels_all[d+1][t][u] = ioffe_sampling(arr_u, node_rnd_vals_all[t]) \n",
    "                \n",
    "    node_embeddings = [{n:[] for n in G.nodes()} for _ in range(depth+1)]\n",
    "    for d in range(depth+1):\n",
    "        for u in G.nodes():\n",
    "            for nl in node_labels_all[d]:\n",
    "                node_embeddings[d][u].append((nl[u][0], nl[u][3], nl[u][1]))\n",
    "    return node_embeddings\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled features\n",
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Elapsed time Nodesketch 23.64945077896118\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vectors_ns_all= nodesketch_iter(G, nodedata, depth=depth, emb_size=emb_size)\n",
    "end = time.time()\n",
    "print('Elapsed time Nodesketch', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_nodesketch_all_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_ns_all[d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled features\n",
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n"
     ]
    }
   ],
   "source": [
    "vectors_ns_reduced = nodesketch_iter(H, nodedata, depth=depth, emb_size=emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_nodesketch_reduced_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_ns_reduced[d], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min-wise (L0) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Iteratively collect the minimum value for each node from its neighbors.\n",
    "# # k iterations correspond to sampling from the k-hop neighborhood\n",
    "# def minwise_iterate(G, rnd_nodes, nr_iter): #, features):\n",
    "#     node_labels = [{} for _ in range(nr_iter+1)]\n",
    "#     if nr_iter < 1:\n",
    "#         raise Exception(\"There must be at least one iteration\")\n",
    "#     node_labels[0] = rnd_nodes\n",
    "#     for iter in range(nr_iter):\n",
    "#         rnd_nodes_iter = node_labels[iter]\n",
    "#         print('Iteration', iter)\n",
    "#         for u in G.nodes():\n",
    "#             w_u = rnd_nodes_iter[u]\n",
    "#             for v in G.neighbors(u):\n",
    "#                 w_u = min(rnd_nodes_iter[v], w_u)\n",
    "#             node_labels[iter+1][u] = w_u\n",
    "#     return node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random numbers for nodes and features for each embedding \n",
    "def init_dicts(nodedata, emb_size):\n",
    "    min_val = 1e-6\n",
    "    cnt = 0\n",
    "    # nodes_rnd = [{} for _ in range(emb_size)]\n",
    "    # labels_rnd = [{} for _ in range(emb_size)]\n",
    "    feats_rnd = [{} for _ in range(emb_size)]\n",
    "    for i in range(emb_size):\n",
    "        # nodes_rnd_i = nodes_rnd[i]\n",
    "        # labels_rnd_i = labels_rnd[i]\n",
    "        feats_rnd_i = feats_rnd[i]\n",
    "        for node, feats in nodedata.items():\n",
    "            # update_dict(nodes_rnd_i, node, rand_gen, min_val)\n",
    "            # update_dict(labels_rnd_i, feats[0], rand_gen, min_val)\n",
    "            for f, weight_f in feats[1].items():\n",
    "                cnt += 1\n",
    "                update_dict(feats_rnd_i, f, min_val, seed=17*cnt)\n",
    "    # return nodes_rnd, labels_rnd, feats_rnd\n",
    "    return feats_rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minwise_samples(G, nodedata, feats_rnd, depth, emb_size):\n",
    "    node_labels = [{} for _ in range(emb_size)]\n",
    "    for i in range(emb_size):\n",
    "        node_labels_i = node_labels[i]\n",
    "        feats_rnd_i = feats_rnd[i]\n",
    "        for node, feats in nodedata.items():\n",
    "            min_feature_value = 1e3\n",
    "            min_feature = None\n",
    "            for f in feats[1]:\n",
    "                if feats_rnd_i[f] < min_feature_value:\n",
    "                    min_feature = f\n",
    "                    min_feature_value = feats_rnd_i[f]\n",
    "            node_labels_i[node] = (min_feature_value, node, feats[0], min_feature)\n",
    "            \n",
    "    node_labels_all = [[{} for _ in range(emb_size)] for _ in range(depth+1)]\n",
    "    node_labels_all[0] = node_labels\n",
    "    for d in range(depth):\n",
    "        node_labels_iter = node_labels_all[d]\n",
    "        print('Iteration', d)\n",
    "        for u in G.nodes():\n",
    "            for t in range(emb_size):\n",
    "                w_u = node_labels_iter[t][u]\n",
    "                for v in G.neighbors(u):\n",
    "                        w_u = min(node_labels_iter[t][v], w_u)\n",
    "                node_labels_all[d+1][t][u] = w_u\n",
    "            \n",
    "    node_embeddings = [{n:[] for n in G.nodes()} for _ in range(depth+1)]\n",
    "    for d in range(depth+1):\n",
    "        for u in G.nodes():\n",
    "            for nl in node_labels_all[d]:\n",
    "                node_embeddings[d][u].append((nl[u][1], nl[u][2], nl[u][3]))\n",
    "    return node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Elapsed time MW 19.19777226448059\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# nodes_rnd, labels_rnd, \n",
    "feats_rnd = init_dicts(nodedata, emb_size)\n",
    "vectors_mw_all = generate_minwise_samples(G, nodedata, feats_rnd, depth=depth, emb_size=emb_size)\n",
    "end = time.time()\n",
    "print('Elapsed time MW', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors_mw_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_minwise_all_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_mw_all[d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n"
     ]
    }
   ],
   "source": [
    "vectors_mw_reduced = generate_minwise_samples(H, nodedata, feats_rnd, depth=depth, emb_size=emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_minwise_reduced_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_mw_reduced[d], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating L1 samples from the k-hop neighborhood\n",
    "# top is the summary size of the frequent items mining algorithm\n",
    "def generate_L1_samples(G, nodedata, depth, emb_size, top):\n",
    "    \n",
    "    \n",
    "    min_val = 1e-6\n",
    "    feats_rnd = [{} for _ in range(emb_size)]\n",
    "    cnt = 0\n",
    "    for i in range(emb_size):\n",
    "        feats_rnd_i = feats_rnd[i]\n",
    "        for node, feats in nodedata.items():\n",
    "            for f, weight_f in feats[1].items():\n",
    "                cnt += 1\n",
    "                update_dict(feats_rnd_i, f, min_val, seed=23*cnt)\n",
    "    \n",
    "    sketches = [[{} for _ in range(emb_size)] for _ in range(depth+1)]\n",
    "    \n",
    "    min_val = 1e-6\n",
    "    cnt = 0\n",
    "    # generate the random values for each node (attribute)\n",
    "    for u, (subject, features) in nodedata.items():\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            print('nodes processed', cnt)\n",
    "        for i in range(emb_size):\n",
    "            feats_rnd_i = feats_rnd[i]\n",
    "            sketches[0][i][u] = {}\n",
    "            max_w = 0\n",
    "            max_f = None\n",
    "            for f, w_f in features.items():\n",
    "                w_rnd = w_f/feats_rnd_i[f]   # get_rnd_value(rand_gen, min_val) \n",
    "                if w_rnd > max_w:\n",
    "                    max_w = w_rnd\n",
    "                    max_f = f\n",
    "            if max_w > 0:\n",
    "                sketches[0][i][u] = {u : (max_w, max_f)}\n",
    "            #print(max_f, max_w)\n",
    "    \n",
    "    # iterate over neighborhoods and maintain the heaviest nodes\n",
    "    for d in range(depth):\n",
    "        print('ITERATION', d)\n",
    "        for emb in range(emb_size):\n",
    "            # print('emb', emb)\n",
    "            sketch_iter_emb = copy.deepcopy(sketches[d][emb])\n",
    "            # print(len(sketch_iter_emb))\n",
    "            new_sketches = {}\n",
    "            for u in G.nodes():\n",
    "                if u not in sketch_iter_emb:\n",
    "                    continue\n",
    "                sketch_u = copy.deepcopy(sketch_iter_emb[u])\n",
    "                for v in G.neighbors(u):\n",
    "                    sketch_v = sketch_iter_emb[v]\n",
    "                    for t, (w_f, f) in sketch_v.items():\n",
    "                        sketch_u.setdefault(t, (0, None))\n",
    "                        weight = sketch_u[t][0] + w_f\n",
    "                        sketch_u[t] = (weight, f)\n",
    "                triples = []\n",
    "                for node, feat_node in sketch_u.items():\n",
    "                    triples.append((feat_node[0], feat_node[1], node))\n",
    "                \n",
    "                # mining heavy hitters\n",
    "                triples = sorted(triples, reverse=True)\n",
    "                to_subtract = 0\n",
    "                if len(triples) > top:\n",
    "                    to_subtract = triples[top][0]\n",
    "                top_triples = triples[:top]\n",
    "                \n",
    "                #print(top_triples)\n",
    "                new_sketches[u] = {tr[2] : (tr[0]-to_subtract, tr[1]) for tr in top_triples}\n",
    "            sketches[d+1][emb] = new_sketches\n",
    "    return sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes processed 1000\n",
      "nodes processed 2000\n",
      "ITERATION 0\n",
      "ITERATION 1\n",
      "ITERATION 2\n",
      "ITERATION 3\n",
      "Elapsed time L1 51.26462650299072\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sketches_l1_all = generate_L1_samples(G, nodedata, depth=depth, emb_size=emb_size, top=top)\n",
    "end = time.time()\n",
    "print('Elapsed time L1', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes processed 1000\n",
      "nodes processed 2000\n",
      "ITERATION 0\n",
      "ITERATION 1\n",
      "ITERATION 2\n",
      "ITERATION 3\n"
     ]
    }
   ],
   "source": [
    "sketches_l1_reduced = generate_L1_samples(H, nodedata, depth=depth, emb_size=emb_size, top=top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sketches_l1_all[2]), len(sketches_l1_reduced[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_l1_2(nodedata, sketches):\n",
    "    embeddings = [{} for _ in range(len(sketches))]\n",
    "    for d in range(len(sketches)):\n",
    "        for node in nodedata.keys():\n",
    "            embeddings[d][node] = []\n",
    "    for d in range(len(sketches)):\n",
    "        for e in range(emb_size):\n",
    "            for node, dct in sketches[d][e].items():\n",
    "                max_word = None\n",
    "                max_weight = 0\n",
    "                for sampled_node, ww in dct.items(): # ww: weight word\n",
    "                    if ww[0] > max_weight:\n",
    "                        max_word = ww[1]\n",
    "                        max_weight = ww[0]\n",
    "                label = nodedata[node][0]\n",
    "                if max_weight > 0:\n",
    "                    embeddings[d][node].append((sampled_node, label, max_word))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_l1_all = get_embeddings_l1_2(nodedata, sketches_l1_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors_l1_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_l1_reduced = get_embeddings_l1_2(nodedata, sketches_l1_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_l1_all_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_l1_all[d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_l1_reduced_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "         json.dump(vectors_l1_reduced[d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating L1 samples from the k-hop neighborhood\n",
    "# top is the summary size of the frequent items mining algorithm\n",
    "def generate_L2_samples(G, nodedata, depth, emb_size, top):\n",
    "    \n",
    "    \n",
    "    min_val = 1e-6\n",
    "    cnt = 0\n",
    "    feats_rnd = [{} for _ in range(emb_size)]\n",
    "    for i in range(emb_size):\n",
    "        feats_rnd_i = feats_rnd[i]\n",
    "        for node, feats in nodedata.items():\n",
    "            for f, weight_f in feats[1].items():\n",
    "                cnt += 1\n",
    "                update_dict(feats_rnd_i, f, min_val, seed=cnt)\n",
    "    \n",
    "    sketches = [[{} for _ in range(emb_size)] for _ in range(depth+1)]\n",
    "    \n",
    "    min_val = 1e-6\n",
    "    cnt = 0\n",
    "    # generate the random values for each node (attribute)\n",
    "    for u, (subject, features) in nodedata.items():\n",
    "        cnt += 1\n",
    "        if cnt % 1000 == 0:\n",
    "            print('nodes processed', cnt)\n",
    "        for i in range(emb_size):\n",
    "            feats_rnd_i = feats_rnd[i]\n",
    "            sketches[0][i][u] = {}\n",
    "            max_w = 0\n",
    "            max_f = None\n",
    "            for f, w_f in features.items():\n",
    "                w_rnd = w_f/math.sqrt(feats_rnd_i[f])   # get_rnd_value(rand_gen, min_val) \n",
    "                if w_rnd > max_w:\n",
    "                    max_w = w_rnd\n",
    "                    max_f = f\n",
    "            if max_w > 0:\n",
    "                sketches[0][i][u] = {u : (max_w, max_f)}\n",
    "            #print(max_f, max_w)\n",
    "    \n",
    "    # iterate over neighborhoods and maintain the heaviest nodes\n",
    "    for d in range(depth):\n",
    "        print('ITERATION', d)\n",
    "        for emb in range(emb_size):\n",
    "            # print('emb', emb)\n",
    "            sketch_iter_emb = copy.deepcopy(sketches[d][emb])\n",
    "            # print(len(sketch_iter_emb))\n",
    "            new_sketches = {}\n",
    "            for u in G.nodes():\n",
    "                if u not in sketch_iter_emb:\n",
    "                    continue\n",
    "                sketch_u = copy.deepcopy(sketch_iter_emb[u])\n",
    "                for v in G.neighbors(u):\n",
    "                    sketch_v = sketch_iter_emb[v]\n",
    "                    for t, (w_f, f) in sketch_v.items():\n",
    "                        sketch_u.setdefault(t, (0, None))\n",
    "                        weight = sketch_u[t][0] + w_f\n",
    "                        sketch_u[t] = (weight, f)\n",
    "                triples = []\n",
    "                for node, feat_node in sketch_u.items():\n",
    "                    triples.append((feat_node[0], feat_node[1], node))\n",
    "                top_triples = sorted(triples, reverse=True)[:top]\n",
    "                #print(top_triples)\n",
    "                new_sketches[u] = {tr[2] : (tr[0], tr[1]) for tr in top_triples}\n",
    "            sketches[d+1][emb] = new_sketches\n",
    "    return sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes processed 1000\n",
      "nodes processed 2000\n",
      "ITERATION 0\n",
      "ITERATION 1\n",
      "ITERATION 2\n",
      "ITERATION 3\n",
      "Elapsed time L2 55.881694316864014\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sketches_l2_all = generate_L2_samples(G, nodedata, depth=depth, emb_size=emb_size, top=top)\n",
    "end = time.time()\n",
    "print('Elapsed time L2', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes processed 1000\n",
      "nodes processed 2000\n",
      "ITERATION 0\n",
      "ITERATION 1\n",
      "ITERATION 2\n",
      "ITERATION 3\n"
     ]
    }
   ],
   "source": [
    "sketches_l2_reduced = generate_L2_samples(H, nodedata, depth=depth, emb_size=emb_size, top=top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_l2_all = get_embeddings_l1_2(nodedata, sketches_l2_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_l2_reduced = get_embeddings_l1_2(nodedata, sketches_l2_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_l2_all_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_l2_all[d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(depth+1):\n",
    "    jsonpath = data_dir + \"/vectors/vectors_l2_reduced_\" + str(emb_size) + \"_hop_\" + str(d) + \".json\"\n",
    "    with open(jsonpath, 'w') as outfile:\n",
    "        json.dump(vectors_l2_reduced[d], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
