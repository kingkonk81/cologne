{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "from BinaryStream import BinaryStream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphname = 'Cora'\n",
    "#graphname = 'Citeseer'\n",
    "emb_size = 400\n",
    "data_dir = os.path.expanduser(\"/home/koki/Desktop/Data/Graphs/\"+graphname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = pd.read_csv(os.path.join(data_dir, graphname.lower() + \".cites\"),\\\n",
    "                       sep='\\t', header=None, names=[\"target\", \"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonpath = os.path.join(data_dir, \"words.json\")\n",
    "with open(jsonpath, 'r') as json_file:\n",
    "    words_file = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "for node, features in words_file.items():\n",
    "    words[str(node)] = features[0], [int(f) for f in features[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = set()\n",
    "labels = set()\n",
    "word_indices = set()\n",
    "for node, features in words.items():\n",
    "    nodes.add(node)\n",
    "    labels.add(features[0])\n",
    "    for w in features[1]:\n",
    "        word_indices.add(str(w))\n",
    "        \n",
    "nodes_path = os.path.join(data_dir, \"graph_nodes.txt\")\n",
    "with open(nodes_path, 'w') as outfile:\n",
    "    for node in nodes:\n",
    "        outfile.write(node + '\\n')\n",
    "        \n",
    "labels_path = os.path.join(data_dir, \"labels.txt\")\n",
    "with open(labels_path, 'w') as outfile:\n",
    "    for label in labels:\n",
    "        outfile.write(label + '\\n')\n",
    "        \n",
    "words_path = os.path.join(data_dir, \"words_indices.txt\")\n",
    "with open(words_path, 'w') as outfile:\n",
    "    for wi in word_indices:\n",
    "        outfile.write(wi + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>103482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>103515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  source\n",
       "0      35    1033\n",
       "1      35  103482\n",
       "2      35  103515"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgelist.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnd_value(stream, min_val):\n",
    "    rval = 0\n",
    "    while rval < min_val:\n",
    "        rval = stream.readUInt64()/(2*sys.maxsize)\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708 5278\n"
     ]
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for idx, edge in edgelist.iterrows():\n",
    "    if str(edge['source']) in words and str(edge['target']) in words:\n",
    "            G.add_edge(str(edge['source']), str(edge['target']))\n",
    "print(G.number_of_nodes(), G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "randompath = \"/home/koki/Desktop/Data/random/merged\"\n",
    "file = open(randompath, 'rb')\n",
    "stream = BinaryStream(file)\n",
    "removed_edges = []\n",
    "H = copy.deepcopy(G)\n",
    "thresh = 0\n",
    "while len(removed_edges) < thresh*G.number_of_edges():\n",
    "    i = random.randint(0, H.number_of_edges())\n",
    "    edge = list(H.edges)[i]\n",
    "    u = edge[0]\n",
    "    v = edge[1]\n",
    "    if H.degree[u] > 1 and H.degree[v] > 1:\n",
    "        H.remove_edge(u, v)\n",
    "        removed_edges.append((u, v))\n",
    "G = copy.deepcopy(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 5278, 0)"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes(), G.number_of_edges(), len(removed_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_path = os.path.join(data_dir, \"all_graph_edges.txt\")\n",
    "with open(edges_path, 'w') as outfile:\n",
    "    for edge in G.edges():\n",
    "        outfile.write(edge[0] + ':' + edge[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(removed_edges) > 0:\n",
    "    removed_edges_path = os.path.join(data_dir, \"removed_edges.txt\")\n",
    "    with open(removed_edges_path, 'w') as outfile:\n",
    "        for edge in removed_edges:\n",
    "            outfile.write(edge[0] + ':' + edge[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk(G, node, depth, features):\n",
    "    node = str(node)\n",
    "    cnt = 0\n",
    "    curr_node = node\n",
    "    while cnt < depth and G.degree[curr_node] > 0:\n",
    "        nbrs = [nbr for nbr in G.neighbors(curr_node)]\n",
    "        curr_node = nbrs[random.randint(0, len(nbrs)-1)]\n",
    "        cnt += 1\n",
    "    subject, features_node = features[curr_node]\n",
    "    return curr_node, subject, features_node[random.randint(0, len(features_node)-1)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1128453', 'Genetic_Algorithms', 1426)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node = list(G.nodes())[23]\n",
    "random_walk(G, node, 2, features=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_nodes_random_walk(G, depth, nr_walks, features):\n",
    "    vectors = {}\n",
    "    for node in G.nodes():\n",
    "        vectors[node] = [None for _ in range(nr_walks)]\n",
    "        for walk in range(nr_walks):\n",
    "            sample, subject, feature = random_walk(G, node, depth, features)\n",
    "            vectors[node][walk] = (sample, subject, feature)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = all_nodes_random_walk(G, 2, emb_size, features=words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('17798', 'Case_Based', 719),\n",
       " ('39126', 'Probabilistic_Methods', 1330),\n",
       " ('39126', 'Probabilistic_Methods', 1137),\n",
       " ('39126', 'Probabilistic_Methods', 1353)]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[list(vectors.keys())[1200]][:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonpath = os.path.join(data_dir, \"vectors_rwalk_all\" + str(emb_size) + \".json\")\n",
    "with open(jsonpath, 'w') as outfile:\n",
    "    json.dump(vectors, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minwise_iterate(G, rnd_nodes, nr_iter, features):\n",
    "    node_labels = [{} for _ in range(nr_iter+1)]\n",
    "    if nr_iter < 1:\n",
    "        raise Exception(\"There must be at least one iteration\")\n",
    "    node_labels[0] = rnd_nodes\n",
    "    for iter in range(nr_iter):\n",
    "        rnd_nodes_iter = node_labels[iter]\n",
    "        print('Iteration', iter)\n",
    "        for u in G.nodes():\n",
    "            w_u = rnd_nodes_iter[u]\n",
    "            for v in G.neighbors(u):\n",
    "                w_u = min(rnd_nodes_iter[v], w_u)\n",
    "            node_labels[iter+1][u] = w_u\n",
    "    return node_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dict(d, k, stream, min_val):\n",
    "    if k not in d:\n",
    "        d[k] = get_rnd_value(stream, min_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Neural_Networks': 0.37134512586272445, 'Rule_Learning': 0.5181951577108905, 'Reinforcement_Learning': 0.9743066396663234, 'Probabilistic_Methods': 0.2127323210294724, 'Theory': 0.06286022729428377, 'Genetic_Algorithms': 0.16900429822561414, 'Case_Based': 0.47654463874962477}, {'Neural_Networks': 0.05481218014561516, 'Rule_Learning': 0.26451992917533296, 'Reinforcement_Learning': 0.5115706218375227, 'Probabilistic_Methods': 0.5044734902599172, 'Theory': 0.02288687022413992, 'Genetic_Algorithms': 0.9044116061216545, 'Case_Based': 0.7894431145449572}]\n"
     ]
    }
   ],
   "source": [
    "# initialize random numbers for nodes and features for each embedding \n",
    "randompath = \"/home/koki/Desktop/Data/random/merged\"\n",
    "file = open(randompath, 'rb')\n",
    "stream = BinaryStream(file)\n",
    "min_val = 1e-6\n",
    "nodes_rnd = [{} for _ in range(emb_size)]\n",
    "cat_rnd = [{} for _ in range(emb_size)]\n",
    "feats_rnd = [{} for _ in range(emb_size)]\n",
    "for i in range(emb_size):\n",
    "    nodes_rnd_i = nodes_rnd[i]\n",
    "    cat_rnd_i = cat_rnd[i]\n",
    "    feats_rnd_i = feats_rnd[i]\n",
    "    for node, feats in words.items():\n",
    "        update_dict(nodes_rnd_i, node, stream, min_val)\n",
    "        update_dict(cat_rnd_i, feats[0], stream, min_val)\n",
    "        for f in feats[1]:\n",
    "            update_dict(feats_rnd_i, f, stream, min_val)\n",
    "print(cat_rnd[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize nodes with a sampled feature\n",
    "node_labels = [{} for _ in range(emb_size)]\n",
    "for i in range(emb_size):\n",
    "    node_labels_i = node_labels[i]\n",
    "    feats_rnd_i = feats_rnd[i]\n",
    "    for node, feats in words.items():\n",
    "        min_feature_value = 1e3\n",
    "        min_feature = None\n",
    "        for f in feats[1]:\n",
    "            if feats_rnd_i[f] < min_feature_value:\n",
    "                min_feature = f\n",
    "                min_feature_value = feats_rnd_i[f]\n",
    "        node_labels_i[node] = (min_feature_value, node, feats[0], min_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n"
     ]
    }
   ],
   "source": [
    "nr_iter = 2\n",
    "node_labels_all = [[{} for _ in range(emb_size)] for _ in range(nr_iter+1)]\n",
    "node_labels_all[0] = node_labels\n",
    "for i in range(nr_iter):\n",
    "    node_labels_iter = node_labels_all[i]\n",
    "    print('Iteration', i)\n",
    "    for u in G.nodes():\n",
    "        for t in range(emb_size):\n",
    "            w_u = node_labels_iter[t][u]\n",
    "            for v in G.neighbors(u):\n",
    "                    w_u = min(node_labels_iter[t][v], w_u)\n",
    "            node_labels_all[i+1][t][u] = w_u\n",
    "            \n",
    "node_embeddings = {n:[] for n in G.nodes()}\n",
    "for u in G.nodes():\n",
    "    for nl in node_labels_all[nr_iter]:\n",
    "        node_embeddings[u].append((nl[u][1], nl[u][2], nl[u][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonpath = os.path.join(data_dir, \"vectors_minwise_all\" + str(emb_size) + \".json\")\n",
    "with open(jsonpath, 'w') as outfile:\n",
    "    json.dump(node_embeddings, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_sketch(sketch, cap, u, w_u_cnt, w_u_over):\n",
    "    sketch_new = copy.deepcopy(sketch)\n",
    "    if u in sketch:\n",
    "        sketch_new[u]['cnt'] += w_u_cnt\n",
    "        sketch_new[u]['over'] += w_u_over\n",
    "    else:\n",
    "        if len(sketch_new) > cap:\n",
    "            min_node = None\n",
    "            min_cnt = -1\n",
    "            for v, est_v in sketch_new.items():\n",
    "                if min_cnt == -1 or est_v['cnt'] <= min_cnt:\n",
    "                    min_node = v\n",
    "                    min_cnt = est_v['cnt']\n",
    "            del sketch_new[min_node]\n",
    "            sketch_new[u] = {'cnt' : w_u_cnt+min_cnt, 'over' : w_u_over+min_cnt}\n",
    "        else:\n",
    "            sketch_new[u] = {'cnt' : w_u_cnt, 'over' : w_u_over}\n",
    "    return sketch_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_L1_samples(G, nodedata, nr_iter, capacity):\n",
    "    sketches = [{} for _ in range(nr_iter+1)]\n",
    "    sketches[0] = {u : {u : {'cnt' : 1/r, 'over' : 0}} for u, r in rnd_nodes.items()}\n",
    "     \n",
    "    printnode = 4\n",
    "    for iter in range(nr_iter):\n",
    "        print('ITERATION', iter)\n",
    "        sketch_iter = copy.deepcopy(sketches[iter])\n",
    "        new_sketches = {}\n",
    "        cnt = 0\n",
    "        for u in G.nodes():\n",
    "            cnt += 1\n",
    "            if cnt % 100 == 0:\n",
    "                print(cnt)\n",
    "            sketch_u = sketch_iter[u]\n",
    "            for v in G.neighbors(u):\n",
    "                sketch_v = sketch_iter[v]\n",
    "                for t, w_t in sketch_v.items():\n",
    "                    sketch_u = update_sketch(sketch_u, capacity, t, w_t['cnt'], w_t['over'])\n",
    "            new_sketches[u] = sketch_u\n",
    "        print('updating sketches', iter+1)\n",
    "        print(len(new_sketches))\n",
    "        sketches[iter+1] = new_sketches\n",
    "    return sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449271 1527938 0.2940374543993277\n",
      "787091 2809188 0.28018452307214753\n",
      "1046503 3840438 0.2724957413711665\n",
      "1220844 4621688 0.2641554341184433\n",
      "1308873 5152938 0.25400519082511763\n",
      "1351180 5434188 0.24864432367816497\n",
      "1356514 5483016 0.2474028892127982\n"
     ]
    }
   ],
   "source": [
    "pairs = 0\n",
    "all_pairs = 0\n",
    "x = 2 # 0: node, 1: label, 2: feature\n",
    "for i in range(len(nodes)):\n",
    "    u = nodes[i]\n",
    "    if (i+1) % 500 == 0:\n",
    "        print(pairs, all_pairs, pairs/all_pairs)\n",
    "    for j in range(i+1, len(nodes)):\n",
    "        v = nodes[j]\n",
    "        all_pairs += 1\n",
    "        c = 0\n",
    "        for t in range(emb_size):\n",
    "            if node_embeddings[u][t][x] == node_embeddings[v][t][x]:\n",
    "                c += 1\n",
    "        if c >= 25:\n",
    "            pairs += 1\n",
    "print(pairs, all_pairs, pairs/all_pairs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('149759', 'AI', 1251),\n",
       " ('nguyen98strict', 'AI', 1623),\n",
       " ('nguyen98strict', 'AI', 1936),\n",
       " ('149759', 'AI', 75),\n",
       " ('nguyen98strict', 'AI', 46)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[list(G.nodes())[180]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('461740', 'ML', 2707),\n",
       " ('nguyen98strict', 'AI', 776),\n",
       " ('461740', 'ML', 1076),\n",
       " ('149759', 'AI', 2568),\n",
       " ('149759', 'AI', 75)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embeddings[list(G.nodes())[180]][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python36864bitd5d6c22048554957af993f0c6b0a4eef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
